import torch
import numpy as np
import gc
import logging
from typing import Optional, Callable, List
from dataclasses import dataclass
from enum import Enum
from silero_vad import load_silero_vad

logger = logging.getLogger(__name__)


class VadState(Enum):
    """VADçŠ¶æ€æšä¸¾"""
    SILENCE = "silence"
    SPEAKING = "speaking"


@dataclass
class SileroVadConfig:
    """Silero VAD é…ç½®å‚æ•°"""
    sample_rate: int = 16000  # æ”¯æŒ 8000 æˆ– 16000
    threshold: float = 0.5  # è¯­éŸ³æ¦‚ç‡é˜ˆå€¼ (0.0-1.0)
    min_speech_duration_ms: int = 250  # æœ€å°è¯­éŸ³æŒç»­æ—¶é•¿
    max_speech_duration_s: float = 10.0  # æœ€å¤§è¯­éŸ³æŒç»­æ—¶é•¿
    min_silence_duration_ms: int = 100  # æœ€å°é™éŸ³æŒç»­æ—¶é•¿ï¼ˆç”¨äºè¿‡æ»¤çŸ­æš‚åœé¡¿ï¼‰
    speech_pad_ms: int = 30  # è¯­éŸ³å‰åå¡«å……æ—¶é•¿
    
    # çª—å£å¤§å°ï¼ˆé‡‡æ ·ç‚¹æ•°ï¼‰- Sileroè¦æ±‚
    window_size_samples: int = 512  # 16000hzç”¨512, 8000hzç”¨256
    
    def __post_init__(self):
        """éªŒè¯é…ç½®"""
        if self.sample_rate not in [8000, 16000]:
            raise ValueError("sample_rate must be 8000 or 16000")
        if not 0.0 <= self.threshold <= 1.0:
            raise ValueError("threshold must be between 0.0 and 1.0")
        
        # éªŒè¯æœ€å°è¯­éŸ³æŒç»­æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
        if self.min_speech_duration_ms < 0:
            raise ValueError("min_speech_duration_ms must be >= 0")
        if self.min_speech_duration_ms > 10000:  # 10ç§’
            raise ValueError("min_speech_duration_ms must be <= 10000 (10 seconds)")
        
        # éªŒè¯æœ€å¤§è¯­éŸ³æŒç»­æ—¶é•¿ï¼ˆç§’ï¼‰
        if self.max_speech_duration_s <= 0:
            raise ValueError("max_speech_duration_s must be > 0")
        if self.max_speech_duration_s > 600:  # 10åˆ†é’Ÿ
            raise ValueError("max_speech_duration_s must be <= 600 (10 minutes)")
        
        # ç¡®ä¿æœ€å¤§æ—¶é•¿å¤§äºæœ€å°æ—¶é•¿
        min_duration_s = self.min_speech_duration_ms / 1000.0
        if self.max_speech_duration_s < min_duration_s:
            raise ValueError(f"max_speech_duration_s ({self.max_speech_duration_s}s) must be >= min_speech_duration_ms ({min_duration_s}s)")
        
        # éªŒè¯æœ€å°é™éŸ³æŒç»­æ—¶é•¿
        if self.min_silence_duration_ms < 0:
            raise ValueError("min_silence_duration_ms must be >= 0")
        
        # éªŒè¯è¯­éŸ³å¡«å……æ—¶é•¿
        if self.speech_pad_ms < 0:
            raise ValueError("speech_pad_ms must be >= 0")
        
        # æ ¹æ®é‡‡æ ·ç‡è‡ªåŠ¨è®¾ç½®çª—å£å¤§å°
        if self.sample_rate == 8000:
            self.window_size_samples = 256
        else:  # 16000
            self.window_size_samples = 512


class VadEvent:
    """VADäº‹ä»¶"""
    def __init__(self, event_type: str, timestamp_ms: float, audio_data: Optional[np.ndarray] = None):
        self.event_type = event_type  # 'speech_start' æˆ– 'speech_end'
        self.timestamp_ms = timestamp_ms
        self.audio_data = audio_data
        
    def __repr__(self):
        audio_info = f", audio_len={len(self.audio_data)}" if self.audio_data is not None else ""
        return f"VadEvent({self.event_type}, {self.timestamp_ms:.0f}ms{audio_info})"


class SileroVAD:
    """
    Silero VAD å°è£…ç±»
    
    ç”¨æ³•ç¤ºä¾‹:
        # åŸºç¡€ä½¿ç”¨
        vad = SileroVAD()
        events = vad.process_audio(pcm_data)
        
        # ä½¿ç”¨å›è°ƒ
        vad = SileroVAD(config=SileroVadConfig(threshold=0.6))
        vad.on_speech_start = lambda ts: print(f"å¼€å§‹è¯´è¯: {ts}ms")
        vad.on_speech_end = lambda ts, audio: print(f"ç»“æŸè¯´è¯: {ts}ms, æ—¶é•¿: {len(audio)/16000:.2f}s")
        
        # æµå¼å¤„ç†
        for chunk in audio_stream:
            events = vad.process_audio(chunk)
            for event in events:
                if event.event_type == 'speech_end':
                    save_audio(event.audio_data)
        
        # æµç»“æŸæ—¶
        final_audio = vad.force_end_speech()
    """
    
    def __init__(self, 
                 config: Optional[SileroVadConfig] = None,
                 use_onnx: bool = True):
        """
        åˆå§‹åŒ– Silero VAD
        
        Args:
            config: VADé…ç½®
            use_onnx: æ˜¯å¦ä½¿ç”¨ONNXç‰ˆæœ¬ï¼ˆæ›´å¿«ï¼Œä½†éœ€è¦onnxruntimeï¼‰
        """
        self.config = config or SileroVadConfig()
        self.use_onnx = use_onnx
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.reset()
        
        # å›è°ƒå‡½æ•°
        self.on_speech_start: Optional[Callable[[float], None]] = None
        self.on_speech_end: Optional[Callable[[float, np.ndarray], None]] = None
        
    def _load_model(self):
        """åŠ è½½ Silero VAD æ¨¡å‹"""
        try:
            self.model = load_silero_vad(onnx=self.use_onnx, opset_version=16)
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def reset(self):
        """é‡ç½®VADçŠ¶æ€"""
        self.state = VadState.SILENCE
        self.buffer = np.array([], dtype=np.float32)
        self.speech_buffer = []
        self.temp_end_buffer = []  # ç”¨äºå­˜å‚¨å¯èƒ½çš„ç»“æŸå‰çš„éŸ³é¢‘
        self.total_samples = 0
        self.speech_start_sample = 0
        self.current_speech_samples = 0
        self.silence_samples = 0
        
        # é‡ç½®æ¨¡å‹çŠ¶æ€ - OnnxWrapperå’ŒPyTorchæ¨¡å‹éƒ½æœ‰reset_statesæ–¹æ³•
        self.model.reset_states()
        
    def _reset_onnx_states(self):
        """é‡ç½®ONNXæ¨¡å‹çŠ¶æ€"""
        self._h = np.zeros((2, 1, 64), dtype=np.float32)
        self._c = np.zeros((2, 1, 64), dtype=np.float32)
        
    def process_audio(self, pcm_data: np.ndarray) -> List[VadEvent]:
        """
        å¤„ç†PCMéŸ³é¢‘æ•°æ®
        
        Args:
            pcm_data: PCMéŸ³é¢‘æ•°æ®ï¼Œnumpyæ•°ç»„ï¼Œfloat32ç±»å‹ï¼Œå€¼åŸŸä¸º[-1, 1]
                     æˆ–int16ç±»å‹ï¼Œå€¼åŸŸä¸º[-32768, 32767]
        
        Returns:
            äº‹ä»¶åˆ—è¡¨
        """
        # å½’ä¸€åŒ–åˆ°[-1, 1]
        if pcm_data.dtype == np.int16:
            pcm_data = pcm_data.astype(np.float32) / 32768.0
        elif pcm_data.dtype != np.float32:
            pcm_data = pcm_data.astype(np.float32)
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self.buffer = np.concatenate([self.buffer, pcm_data])
        events = []
        
        # æŒ‰çª—å£å¤§å°å¤„ç†
        window_size = self.config.window_size_samples
        while len(self.buffer) >= window_size:
            window = self.buffer[:window_size]
            self.buffer = self.buffer[window_size:]
            
            event = self._process_window(window)
            if event is not None:
                events.append(event)
        
        return events
    
    def _process_window(self, window: np.ndarray) -> Optional[VadEvent]:
        """å¤„ç†å•ä¸ªéŸ³é¢‘çª—å£"""
        self.total_samples += len(window)
        timestamp_ms = (self.total_samples / self.config.sample_rate) * 1000
        
        # è·å–è¯­éŸ³æ¦‚ç‡
        speech_prob = self._get_speech_probability(window)
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºè¯­éŸ³
        is_speech = speech_prob >= self.config.threshold
        
        event = None
        min_speech_samples = int(self.config.min_speech_duration_ms * self.config.sample_rate / 1000)
        min_silence_samples = int(self.config.min_silence_duration_ms * self.config.sample_rate / 1000)
        max_speech_samples = int(self.config.max_speech_duration_s * self.config.sample_rate)
        
        if self.state == VadState.SILENCE:
            if is_speech:
                self.speech_buffer.append(window)
                self.current_speech_samples += len(window)
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å°è¯­éŸ³æ—¶é•¿
                if self.current_speech_samples >= min_speech_samples:
                    self.state = VadState.SPEAKING
                    self.speech_start_sample = self.total_samples - self.current_speech_samples
                    
                    event = VadEvent('speech_start', 
                                   (self.speech_start_sample / self.config.sample_rate) * 1000)
                    if self.on_speech_start:
                        self.on_speech_start(event.timestamp_ms)
                    
                    self.silence_samples = 0
            else:
                # é‡ç½®è¯­éŸ³ç¼“å†²
                if self.current_speech_samples > 0:
                    self.speech_buffer = []
                    self.current_speech_samples = 0
        
        elif self.state == VadState.SPEAKING:
            self.speech_buffer.append(window)
            self.current_speech_samples += len(window)
            
            if is_speech:
                # ç»§ç»­è¯­éŸ³
                self.silence_samples = 0
                self.temp_end_buffer = []
                
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§è¯­éŸ³æ—¶é•¿
                if self.current_speech_samples >= max_speech_samples:
                    event = self._end_speech(timestamp_ms)
            else:
                # å¯èƒ½çš„è¯­éŸ³ç»“æŸ
                self.silence_samples += len(window)
                self.temp_end_buffer.append(window)
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å°é™éŸ³æ—¶é•¿
                if self.silence_samples >= min_silence_samples:
                    # ç§»é™¤æœ«å°¾çš„é™éŸ³éƒ¨åˆ†
                    if self.temp_end_buffer:
                        for _ in range(len(self.temp_end_buffer)):
                            if self.speech_buffer:
                                self.speech_buffer.pop()
                    
                    event = self._end_speech(timestamp_ms)
        
        return event
    
    def _end_speech(self, timestamp_ms: float) -> Optional[VadEvent]:
        """ç»“æŸå½“å‰è¯­éŸ³ç‰‡æ®µ"""
        # æ·»åŠ padding
        pad_samples = int(self.config.speech_pad_ms * self.config.sample_rate / 1000)
        
        # åˆå¹¶è¯­éŸ³ç‰‡æ®µ
        if self.speech_buffer:
            speech_audio = np.concatenate(self.speech_buffer)
            
            # æ·»åŠ åç½®paddingï¼ˆå¦‚æœç¼“å†²åŒºè¿˜æœ‰æ•°æ®ï¼‰
            if len(self.buffer) > 0:
                pad_end = min(pad_samples, len(self.buffer))
                speech_audio = np.concatenate([speech_audio, self.buffer[:pad_end]])
        else:
            speech_audio = np.array([], dtype=np.float32)
        
        # è®¡ç®—éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
        audio_duration_s = len(speech_audio) / self.config.sample_rate
        min_duration_s = self.config.min_speech_duration_ms / 1000.0
        max_duration_s = self.config.max_speech_duration_s
        
        # æ£€æŸ¥æ˜¯å¦å°äºæœ€å°è¯­éŸ³æ—¶é•¿
        if audio_duration_s < min_duration_s:
            logger.debug(f"è¯­éŸ³æ—¶é•¿è¿‡çŸ­ ({audio_duration_s:.3f}ç§’ < {min_duration_s:.3f}ç§’)ï¼Œå·²ä¸¢å¼ƒ")
            # é‡ç½®çŠ¶æ€ä½†ä¸è§¦å‘äº‹ä»¶
            self.state = VadState.SILENCE
            self.speech_buffer = []
            self.temp_end_buffer = []
            self.current_speech_samples = 0
            self.silence_samples = 0
            return None
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§è¯­éŸ³æ—¶é•¿ï¼Œå¦‚æœè¶…è¿‡åˆ™è£å‰ª
        if audio_duration_s > max_duration_s:
            max_samples = int(max_duration_s * self.config.sample_rate)
            speech_audio = speech_audio[:max_samples]
            logger.debug(f"è¯­éŸ³æ—¶é•¿è¶…è¿‡æœ€å¤§å€¼ ({audio_duration_s:.3f}ç§’ > {max_duration_s:.3f}ç§’)ï¼Œå·²è£å‰ªåˆ° {max_duration_s:.3f}ç§’")
        
        event = VadEvent('speech_end', timestamp_ms, speech_audio)
        
        if self.on_speech_end:
            self.on_speech_end(timestamp_ms, speech_audio)
        
        # é‡ç½®çŠ¶æ€
        self.state = VadState.SILENCE
        self.speech_buffer = []
        self.temp_end_buffer = []
        self.current_speech_samples = 0
        self.silence_samples = 0
        
        return event
    
    def _get_speech_probability(self, window: np.ndarray) -> float:
        """è·å–è¯­éŸ³æ¦‚ç‡ - ç»Ÿä¸€æ¥å£ï¼ŒOnnxWrapperå’ŒPyTorchæ¨¡å‹æ¥å£ç›¸åŒ"""
        with torch.no_grad():
            audio_tensor = torch.from_numpy(window).unsqueeze(0)  # (1, samples)
            speech_prob = self.model(audio_tensor, self.config.sample_rate).item()
        return speech_prob
    
    def force_end_speech(self) -> Optional[np.ndarray]:
        """
        å¼ºåˆ¶ç»“æŸå½“å‰è¯­éŸ³ç‰‡æ®µï¼ˆç”¨äºæµç»“æŸæ—¶ï¼‰
        
        Returns:
            å½“å‰çš„è¯­éŸ³æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
        """
        if self.state == VadState.SPEAKING and self.speech_buffer:
            timestamp_ms = (self.total_samples / self.config.sample_rate) * 1000
            event = self._end_speech(timestamp_ms)
            if event is not None:
                return event.audio_data
        
        return None
    
    def get_current_state(self) -> VadState:
        """è·å–å½“å‰VADçŠ¶æ€"""
        return self.state
    
    def get_speech_duration_ms(self) -> float:
        """è·å–å½“å‰è¯­éŸ³ç‰‡æ®µçš„æŒç»­æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰"""
        if self.state == VadState.SPEAKING:
            return (self.current_speech_samples / self.config.sample_rate) * 1000
        return 0.0
    
    def close(self):
        """å…³é—­VADå¹¶é‡Šæ”¾æ¨¡å‹èµ„æº"""
        try:
            # å…ˆæ¸…ç†æ‰€æœ‰ç¼“å†²åŒºå’ŒçŠ¶æ€ï¼ˆåœ¨åˆ é™¤æ¨¡å‹ä¹‹å‰ï¼‰
            if hasattr(self, 'buffer'):
                self.buffer = np.array([], dtype=np.float32)
            if hasattr(self, 'speech_buffer'):
                self.speech_buffer.clear()
            if hasattr(self, 'temp_end_buffer'):
                self.temp_end_buffer.clear()
            
            # é‡ç½®çŠ¶æ€ï¼ˆéœ€è¦åœ¨åˆ é™¤æ¨¡å‹ä¹‹å‰ï¼Œå› ä¸ºresetä¼šè°ƒç”¨model.reset_states()ï¼‰
            if hasattr(self, 'model') and self.model is not None:
                try:
                    self.model.reset_states()
                except Exception:
                    pass  # å¦‚æœreset_stateså¤±è´¥ï¼Œç»§ç»­æ¸…ç†
            
            # æ˜¾å¼é‡Šæ”¾æ¨¡å‹èµ„æº
            if hasattr(self, 'model') and self.model is not None:
                try:
                    del self.model
                except Exception:
                    pass
                self.model = None
            
            # é‡ç½®å…¶ä»–çŠ¶æ€å˜é‡
            if hasattr(self, 'state'):
                self.state = VadState.SILENCE
            if hasattr(self, 'total_samples'):
                self.total_samples = 0
            if hasattr(self, 'speech_start_sample'):
                self.speech_start_sample = 0
            if hasattr(self, 'current_speech_samples'):
                self.current_speech_samples = 0
            if hasattr(self, 'silence_samples'):
                self.silence_samples = 0
            
            # æ‰‹åŠ¨è§¦å‘åƒåœ¾å›æ”¶
            gc.collect()
            
            # å¦‚æœæ¨¡å‹åœ¨ CUDA ä¸Šï¼Œæ¸…ç† GPU ç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            logger.info("Silero VAD æ¨¡å‹å·²å…³é—­å¹¶é‡Šæ”¾å†…å­˜")
        except Exception as e:
            logger.error(f"å…³é—­ Silero VAD å¤±è´¥: {e}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import time
    
    print("=" * 60)
    print("Silero VAD ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    # åˆ›å»ºVADå®ä¾‹
    config = SileroVadConfig(
        sample_rate=16000,
        threshold=0.5,  # å¯ä»¥æ ¹æ®ç¯å¢ƒè°ƒæ•´ (0.3-0.7)
        min_silence_duration_ms=500,  # 500msé™éŸ³è®¤ä¸ºç»“æŸ
        min_speech_duration_ms=250,   # 250msè¯­éŸ³æ‰ç®—å¼€å§‹
        speech_pad_ms=30
    )
    
    vad = SileroVAD(config=config, use_onnx=False)  # æ”¹ä¸ºTrueä½¿ç”¨ONNXç‰ˆæœ¬ï¼ˆæ¨èï¼‰
    
    # è®¾ç½®å›è°ƒ
    def on_start(ts):
        print(f"ğŸ¤ è¯­éŸ³å¼€å§‹: {ts:.0f}ms")
    
    def on_end(ts, audio):
        duration = len(audio) / config.sample_rate
        print(f"ğŸ”‡ è¯­éŸ³ç»“æŸ: {ts:.0f}ms, æ—¶é•¿: {duration:.2f}ç§’, æ ·æœ¬æ•°: {len(audio)}")
    
    vad.on_speech_start = on_start
    vad.on_speech_end = on_end
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    print("\nç”Ÿæˆæµ‹è¯•éŸ³é¢‘...")
    silence = np.random.randn(16000) * 0.001  # 1ç§’é™éŸ³
    speech = np.random.randn(32000) * 0.1     # 2ç§’è¯­éŸ³
    
    test_audio = np.concatenate([
        silence,
        speech,
        silence * 0.5,  # 0.5ç§’é™éŸ³
        speech * 1.2,   # 2ç§’è¯­éŸ³ï¼ˆç¨å¼ºï¼‰
        silence
    ]).astype(np.float32)
    
    print(f"æ€»éŸ³é¢‘é•¿åº¦: {len(test_audio)/16000:.2f}ç§’")
    print("\nå¼€å§‹å¤„ç†...\n")
    
    # åˆ†å—å¤„ç†ï¼ˆæ¨¡æ‹Ÿå®æ—¶æµï¼‰
    chunk_size = 512  # 32ms chunks
    start_time = time.time()
    
    for i in range(0, len(test_audio), chunk_size):
        chunk = test_audio[i:i+chunk_size]
        events = vad.process_audio(chunk)
    
    # æµç»“æŸæ—¶å¼ºåˆ¶ç»“æŸ
    final_audio = vad.force_end_speech()
    if final_audio is not None:
        print(f"âš ï¸  å¼ºåˆ¶ç»“æŸæœ€åç‰‡æ®µ: {len(final_audio)/16000:.2f}ç§’")
    
    elapsed = time.time() - start_time
    print(f"\nå¤„ç†å®Œæˆï¼Œç”¨æ—¶: {elapsed:.3f}ç§’")
    print(f"å®æ—¶ç‡: {len(test_audio)/16000/elapsed:.2f}x")
    
    print("\n" + "=" * 60)
    print("æç¤º:")
    print("- threshold è¶Šé«˜è¶Šä¸¥æ ¼ï¼Œå»ºè®® 0.4-0.6")
    print("- å™ªå£°ç¯å¢ƒå¯ä»¥é™ä½åˆ° 0.3")
    print("- ä½¿ç”¨ use_onnx=True å¯è·å¾—æ›´å¿«çš„æ€§èƒ½")
    print("=" * 60)