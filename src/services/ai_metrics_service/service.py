#!/usr/bin/env python3
"""
AIæŒ‡æ ‡æœåŠ¡ä¸»ç±»

åŸºäºUTCPåè®®å®ç°çš„AIæ¨¡å‹è°ƒç”¨æ€§èƒ½ç›‘æ§å’Œè´¹ç”¨ç»Ÿè®¡æœåŠ¡ã€‚
"""

import asyncio
import logging
import time
import uuid
from typing import List, Dict, Any, Optional, Callable
from functools import wraps
from collections import deque

from src.utcp.utcp import UTCPService
from src.common import ConfigManager
from src.services.ai_metrics_service.calculator import CostCalculator
from src.services.ai_metrics_service.persistence import DatabasePersistence
from src.services.ai_metrics_service.models import CallMetrics
from src.services.ai_metrics_service.exceptions import AIMetricsError, MonitoringError, CostCalculationError

logger = logging.getLogger(__name__)


def require_db_initialized(func: Callable) -> Callable:
    """è£…é¥°å™¨ï¼šç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–"""
    @wraps(func)
    async def wrapper(self, *args, **kwargs):
        if not self._db_initialized:
            await self.data_persistence.initialize()
            self._db_initialized = True
        return await func(self, *args, **kwargs)
    return wrapper


def handle_errors(error_type: type = AIMetricsError) -> Callable:
    """è£…é¥°å™¨ï¼šç»Ÿä¸€é”™è¯¯å¤„ç†"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            try:
                return await func(self, *args, **kwargs)
            except Exception as e:
                logger.error(f"{func.__name__} å¤±è´¥: {e}")
                raise error_type(f"{func.__name__} å¤±è´¥: {e}")
        return wrapper
    return decorator


class AIMetricsService(UTCPService):
    """AIæŒ‡æ ‡æœåŠ¡ä¸»ç±»"""
    # æ’ä»¶ä¸å…è®¸å†™__init__æ–¹æ³•ï¼Œåªèƒ½é€šè¿‡initæ–¹æ³•è¿›è¡Œåˆå§‹åŒ–

    def init(self) -> None:
        """æ’ä»¶åˆå§‹åŒ–æ–¹æ³•"""
        self.service_config = self.config
        self._init_components()

    def _init_components(self):
        """åˆå§‹åŒ–æœåŠ¡ç»„ä»¶"""
        # ä½¿ç”¨åˆå¹¶åçš„é…ç½®
        cost_calculation_config = self.service_config.get("cost_calculation", {})
        custom_pricing = cost_calculation_config.get("custom_pricing", {})
        
        # åˆå§‹åŒ–è´¹ç”¨è®¡ç®—å™¨
        self.cost_calculator = CostCalculator(
            custom_pricing=custom_pricing
        )
        
        # åˆå§‹åŒ–æ•°æ®åº“æŒä¹…åŒ–ç»„ä»¶
        self.data_persistence = DatabasePersistence(self.config_manager)
        
        # æ ‡è®°ä¸ºæœªåˆå§‹åŒ–çŠ¶æ€
        self._db_initialized = False
        
        # ç®€å•çš„ä¼šè¯å­˜å‚¨ï¼ˆæ›¿ä»£collectorï¼‰
        self._active_sessions: Dict[str, Dict[str, Any]] = {}
        
        # æ‰¹é‡æ’å…¥é˜Ÿåˆ—é…ç½®
        batch_config = self.service_config.get("batch_insert", {})
        self._batch_size = batch_config.get("batch_size", 10)  # é»˜è®¤æ¯æ‰¹10æ¡
        self._batch_timeout = batch_config.get("batch_timeout", 5.0)  # é»˜è®¤5ç§’è¶…æ—¶
        self._metrics_queue: deque = deque()
        self._last_batch_time = time.time()
        self._batch_task: Optional[asyncio.Task] = None
        self._queue_lock = asyncio.Lock()
    
    @property
    def name(self) -> str:
        """æœåŠ¡åç§°"""
        return "ai_metrics_service"
    
    @property
    def description(self) -> str:
        """æœåŠ¡æè¿°"""
        return "AIæ¨¡å‹è°ƒç”¨æ€§èƒ½ç›‘æ§å’Œè´¹ç”¨ç»Ÿè®¡æœåŠ¡ï¼ˆæ•°æ®åº“ç‰ˆæœ¬ï¼‰"
    
    def _create_tool_definition(self, name: str, description: str, 
                               properties: Dict[str, Any], required: List[str] = None) -> Dict[str, Any]:
        """åˆ›å»ºå·¥å…·å®šä¹‰çš„è¾…åŠ©æ–¹æ³•"""
        return {
            "type": "function",
            "function": {
                "name": name,
                "description": description,
                "parameters": {
                    "type": "object",
                    "properties": properties,
                    "required": required or []
                }
            }
        }
    
    async def get_tools(self) -> List[Dict[str, Any]]:
        """è¿”å›å¯ç”¨å·¥å…·åˆ—è¡¨"""
        tools = [
            # ç›‘æ§ç›¸å…³å·¥å…·
            self._create_tool_definition(
                "start_monitoring", "å¼€å§‹ç›‘æ§",
                {"model_name": {"type": "string", "description": "æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰"}}
            ),
            
            # ç›‘æ§å®Œæˆå·¥å…·
            self._create_tool_definition(
                "finish_monitoring", "å®Œæˆç›‘æ§å¹¶ä¿å­˜è®°å½•åˆ°æ•°æ®åº“",
                {
                    "monitor_id": {"type": "string", "description": "ç›‘æ§ID"},
                    "model_name": {"type": "string", "description": "æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰"},
                    "provider": {"type": "string", "description": "æä¾›å•†ï¼ˆå¯é€‰ï¼Œé»˜è®¤unknownï¼‰"},
                    "session_id": {"type": "string", "description": "ä¼šè¯IDï¼ˆå¯é€‰ï¼‰"},
                    "prompt_tokens": {"type": "integer", "description": "è¾“å…¥tokenæ•°é‡"},
                    "completion_tokens": {"type": "integer", "description": "è¾“å‡ºtokenæ•°é‡"},
                    "input_chars": {"type": "integer", "description": "è¾“å…¥å­—ç¬¦æ•°"},
                    "output_chars": {"type": "integer", "description": "è¾“å‡ºå­—ç¬¦æ•°"},
                    "tool_count": {"type": "integer", "description": "å·¥å…·æ•°é‡"},
                    "tool_calls_made": {"type": "integer", "description": "å·¥å…·è°ƒç”¨æ¬¡æ•°"},
                    "http_first_byte_time": {"type": "number", "description": "HTTPé¦–å­—èŠ‚æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰"},
                    "first_token_time": {"type": "number", "description": "ç¬¬ä¸€ä¸ªtokenæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰"},
                    "result": {"type": "string", "description": "è°ƒç”¨ç»“æœï¼ˆå¯é€‰ï¼‰"}
                },
                ["monitor_id"]
            ),
            
            # å–æ¶ˆç›‘æ§å·¥å…·
            self._create_tool_definition(
                "cancel_monitor", "å–æ¶ˆç›‘æ§ä¼šè¯ï¼ˆå½“å‡ºç°é”™è¯¯æ—¶ä½¿ç”¨ï¼‰",
                {"monitor_id": {"type": "string", "description": "ç›‘æ§ID"}},
                ["monitor_id"]
            ),
            
            # æ•°æ®æŸ¥è¯¢å·¥å…·
            self._create_tool_definition(
                "get_statistics", "è·å–ç»Ÿè®¡æ•°æ®",
                {
                    "model_name": {"type": "string", "description": "æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰"},
                            "period": {
                                "type": "string",
                        "description": "ç»Ÿè®¡å‘¨æœŸï¼šhour/day/week/month",
                        "enum": ["hour", "day", "week", "month"]
                    }
                }
            ),
            self._create_tool_definition(
                "load_historical_data", "åŠ è½½å†å²æ•°æ®",
                {
                    "model_name": {"type": "string", "description": "æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰"},
                    "start_time": {"type": "number", "description": "å¼€å§‹æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰"},
                    "end_time": {"type": "number", "description": "ç»“æŸæ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰"},
                    "limit": {"type": "integer", "description": "è¿”å›è®°å½•æ•°é‡é™åˆ¶ï¼ˆé»˜è®¤100ï¼‰"}
                }
            ),
            
            # æ•°æ®ç»´æŠ¤å·¥å…·
            self._create_tool_definition(
                "cleanup_old_data", "æ¸…ç†æ—§æ•°æ®",
                {"max_days": {"type": "integer", "description": "ä¿ç•™å¤©æ•°ï¼ˆé»˜è®¤30å¤©ï¼‰"}}
            ),
            
            
            # ç³»ç»Ÿä¿¡æ¯å·¥å…·
            self._create_tool_definition(
                "get_data_info", "è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯", {}
            )
        ]
        
        return tools
    
    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Any:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        # å·¥å…·æ˜ å°„è¡¨
        tool_handlers = {
            "start_monitoring": lambda: self.start_monitoring(),
            "finish_monitoring": lambda: self.finish_monitoring(
                arguments["monitor_id"],
                arguments.get("provider"),
                arguments.get("model_name"),
                arguments.get("session_id"),
                arguments.get("prompt_tokens", 0),
                arguments.get("completion_tokens", 0),
                arguments.get("input_chars", 0),
                arguments.get("output_chars", 0),
                arguments.get("tool_count", 0),
                arguments.get("tool_calls_made", 0),
                arguments.get("http_first_byte_time"),
                arguments.get("first_token_time"),
                arguments.get("result")
            ),
            "cancel_monitor": lambda: self.cancel_monitor(arguments["monitor_id"]),
            "get_statistics": lambda: self.get_statistics(
                arguments.get("model_name"), arguments.get("period", "day")
            ),
            "load_historical_data": lambda: self.load_historical_data(
                arguments.get("model_name"), arguments.get("start_time"),
                arguments.get("end_time"), arguments.get("limit", 100)
            ),
            "cleanup_old_data": lambda: self.cleanup_old_data(arguments.get("max_days", 30)),
            "get_data_info": lambda: self.get_data_info()
        }
        
        try:
            if tool_name not in tool_handlers:
                raise ValueError(f"æœªçŸ¥çš„å·¥å…·åç§°: {tool_name}")
            
            return await tool_handlers[tool_name]()
        except Exception as e:
            logger.error(f"æ‰§è¡Œå·¥å…· '{tool_name}' æ—¶å‡ºé”™: {e}")
            return {
                "status": "error",
                "error": str(e),
                "message": f"æ‰§è¡Œå·¥å…· '{tool_name}' å¤±è´¥"
            }

    @handle_errors(MonitoringError)
    async def start_monitoring(self) -> Dict[str, Any]:
        """å¼€å§‹ç›‘æ§"""
        monitor_id = str(uuid.uuid4())
        self._active_sessions[monitor_id] = {
            "start_time": time.time()
        }
        return {
            "monitor_id": monitor_id,
            "status": "started"
        }
    
    @handle_errors(MonitoringError)
    async def finish_monitoring(self, monitor_id: str,
                               provider: str = None,
                               model_name: str = None, 
                               session_id: str = None,
                               prompt_tokens: int = 0,
                               completion_tokens: int = 0,
                               input_chars: int = 0,
                               output_chars: int = 0,
                               tool_count: int = 0,
                               tool_calls_made: int = 0,
                               http_first_byte_time: float = None,
                               first_token_time: float = None,
                               result: str = None) -> Dict[str, Any]:
        """å®Œæˆç›‘æ§å¹¶ä¿å­˜è®°å½•åˆ°æ•°æ®åº“"""
        # è·å–ä¼šè¯ä¿¡æ¯
        if monitor_id not in self._active_sessions:
            raise MonitoringError(f"ç›‘æ§ä¼šè¯ä¸å­˜åœ¨: {monitor_id}")
        
        session_info = self._active_sessions[monitor_id]
        start_time = session_info["start_time"]
        actual_model_name = model_name
        
        # æ¸…ç†ä¼šè¯æ•°æ®
        del self._active_sessions[monitor_id]

        # ç›´æ¥åˆ›å»ºCallMetricså¯¹è±¡
        metrics = CallMetrics(
            monitor_id=monitor_id,
            provider=provider,
            model_name=actual_model_name,
            session_id=session_id,
            start_time=start_time,
            end_time=time.time(),
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            input_chars=input_chars,
            output_chars=output_chars,
            tool_count=tool_count,
            tool_calls_made=tool_calls_made,
            http_first_byte_time=http_first_byte_time,
            first_token_time=first_token_time,
            result=result
        )
        
        # è®¡ç®—è´¹ç”¨
        total_cost = self.cost_calculator.calculate_cost(model_name, metrics.prompt_tokens, metrics.completion_tokens)
        
        # è®¡ç®—è¾“å…¥å’Œè¾“å‡ºè´¹ç”¨
        model_pricing = self.cost_calculator.get_model_pricing(model_name)
        input_cost = metrics.prompt_tokens * model_pricing.get("input_cost_per_token", 0)
        output_cost = metrics.completion_tokens * model_pricing.get("output_cost_per_token", 0)
        
        metrics.cost = total_cost
        metrics.input_cost = input_cost
        metrics.output_cost = output_cost
        
        # æ·»åŠ åˆ°æ‰¹é‡æ’å…¥é˜Ÿåˆ—ï¼ˆä¸é˜»å¡ï¼‰
        asyncio.create_task(self._add_to_batch_queue(metrics))
        
        self.logger.debug(f"ğŸ“Š æŒ‡æ ‡æ•°æ®å·²æäº¤ä¿å­˜: "
                        f"total_time={metrics.total_time:.2f}ms, "
                        f"prompt_tokens={metrics.prompt_tokens}, "
                        f"completion_tokens={metrics.completion_tokens}, "
                        f"output_chars={metrics.output_chars}, "
                        f"tool_calls_made={metrics.tool_calls_made}")

        return {
            "monitor_id": monitor_id,
            "model_name": metrics.model_name,
            "provider": metrics.provider,
            "session_id": session_id,
            "result": result,
            "status": "finished_and_saved",
            "cost": metrics.cost,
            "metrics": metrics.to_dict()
        }

    @handle_errors(MonitoringError)
    async def cancel_monitor(self, monitor_id: str) -> Dict[str, Any]:
        """å–æ¶ˆç›‘æ§ä¼šè¯ï¼ˆå½“å‡ºç°é”™è¯¯æ—¶ä½¿ç”¨ï¼‰"""
        # æ£€æŸ¥ç›‘æ§ä¼šè¯æ˜¯å¦å­˜åœ¨
        if monitor_id not in self._active_sessions:
            raise MonitoringError(f"ç›‘æ§ä¼šè¯ä¸å­˜åœ¨: {monitor_id}")
        
        # è·å–ä¼šè¯ä¿¡æ¯
        session_info = self._active_sessions[monitor_id]
        start_time = session_info["start_time"]
        
        # æ¸…ç†ä¼šè¯æ•°æ®
        del self._active_sessions[monitor_id]
        
        # è®°å½•å–æ¶ˆä¿¡æ¯
        logger.info(f"ç›‘æ§ä¼šè¯å·²å–æ¶ˆ: {monitor_id}, æŒç»­æ—¶é—´: {time.time() - start_time:.2f}ç§’")
        
        return {
            "monitor_id": monitor_id,
            "status": "cancelled",
            "duration": time.time() - start_time,
            "message": "ç›‘æ§ä¼šè¯å·²æˆåŠŸå–æ¶ˆ"
        }

    @require_db_initialized
    @handle_errors(AIMetricsError)
    async def get_statistics(self, model_name: str = None, period: str = "day") -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡æ•°æ®"""
        return await self.data_persistence.get_statistics(model_name, period)

    @require_db_initialized
    @handle_errors(AIMetricsError)
    async def load_historical_data(self, model_name: str = None, start_time: float = None,
                                 end_time: float = None, limit: int = 100) -> List[Dict[str, Any]]:
        """åŠ è½½å†å²æ•°æ®"""
        metrics_list = await self.data_persistence.load_historical_data(
            model_name, start_time, end_time, limit
        )
        return [metrics.to_dict() for metrics in metrics_list]

    @require_db_initialized
    @handle_errors(AIMetricsError)
    async def cleanup_old_data(self, max_days: int = 30) -> Dict[str, Any]:
        """æ¸…ç†æ—§æ•°æ®"""
        cleaned_count = await self.data_persistence.cleanup_old_data(max_days)
        return {
            "cleaned_count": cleaned_count,
            "max_days": max_days,
            "status": "completed"
        }


    @require_db_initialized
    @handle_errors(AIMetricsError)
    async def get_data_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        return await self.data_persistence.get_data_info()

    async def _ensure_db_initialized(self):
        """ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–"""
        if not self._db_initialized:
            await self.data_persistence.initialize()
            self._db_initialized = True
    
    async def _add_to_batch_queue(self, metrics: CallMetrics):
        """å°†æŒ‡æ ‡æ·»åŠ åˆ°æ‰¹é‡æ’å…¥é˜Ÿåˆ—"""
        should_flush = False
        should_start_timer = False
        
        async with self._queue_lock:
            self._metrics_queue.append(metrics)
            
            # å¦‚æœé˜Ÿåˆ—è¾¾åˆ°æ‰¹é‡å¤§å°ï¼Œç«‹å³è§¦å‘æ‰¹é‡ä¿å­˜
            if len(self._metrics_queue) >= self._batch_size:
                should_flush = True
            else:
                # éœ€è¦å¯åŠ¨æˆ–é‡ç½®å®šæ—¶å™¨ä»»åŠ¡
                should_start_timer = True
        
        # åœ¨é”å¤–æ‰§è¡Œæ‰¹é‡ä¿å­˜å’Œå®šæ—¶å™¨æ“ä½œ
        if should_flush:
            await self._flush_batch_queue()
        elif should_start_timer:
            # å¯åŠ¨æˆ–é‡ç½®å®šæ—¶å™¨ä»»åŠ¡ï¼ˆåœ¨é”å¤–æ‰§è¡Œï¼Œé¿å…æ­»é”ï¼‰
            self._start_batch_timer()
    
    def _start_batch_timer(self):
        """å¯åŠ¨æ‰¹é‡ä¿å­˜å®šæ—¶å™¨ï¼ˆéé˜»å¡ï¼‰"""
        # å–æ¶ˆä¹‹å‰çš„ä»»åŠ¡
        if self._batch_task and not self._batch_task.done():
            self._batch_task.cancel()
        
        # åˆ›å»ºæ–°çš„å®šæ—¶ä»»åŠ¡
        self._batch_task = asyncio.create_task(self._batch_timer_task())
    
    async def _batch_timer_task(self):
        """æ‰¹é‡ä¿å­˜å®šæ—¶å™¨ä»»åŠ¡"""
        try:
            await asyncio.sleep(self._batch_timeout)
            # æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦è¿˜æœ‰æ•°æ®éœ€è¦ä¿å­˜
            async with self._queue_lock:
                if self._metrics_queue:
                    await self._flush_batch_queue()
        except asyncio.CancelledError:
            # ä»»åŠ¡è¢«å–æ¶ˆæ˜¯æ­£å¸¸çš„ï¼ˆå½“é˜Ÿåˆ—è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶ï¼‰
            pass
    
    async def _flush_batch_queue(self):
        """åˆ·æ–°æ‰¹é‡é˜Ÿåˆ—ï¼Œæ‰§è¡Œæ‰¹é‡æ’å…¥"""
        # åœ¨é”å†…å–å‡ºé˜Ÿåˆ—æ•°æ®
        async with self._queue_lock:
            if not self._metrics_queue:
                return
            
            # å–å‡ºé˜Ÿåˆ—ä¸­çš„æ‰€æœ‰æŒ‡æ ‡
            metrics_list = list(self._metrics_queue)
            self._metrics_queue.clear()
            self._last_batch_time = time.time()
            
            # å–æ¶ˆå®šæ—¶å™¨ä»»åŠ¡ï¼ˆå¦‚æœè¿˜åœ¨è¿è¡Œï¼‰
            if self._batch_task and not self._batch_task.done():
                self._batch_task.cancel()
                self._batch_task = None
        
        # åœ¨é”å¤–æ‰§è¡Œæ‰¹é‡ä¿å­˜ï¼ˆé¿å…é•¿æ—¶é—´æŒæœ‰é”ï¼‰
        try:
            await self._ensure_db_initialized()
            saved_count = await self.data_persistence.save_metrics_batch(metrics_list)
            self.logger.debug(f"âœ… æ‰¹é‡ä¿å­˜æŒ‡æ ‡æ•°æ®æˆåŠŸ: {saved_count}/{len(metrics_list)} æ¡è®°å½•")
        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡ä¿å­˜æŒ‡æ ‡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            # å¦‚æœæ‰¹é‡ä¿å­˜å¤±è´¥ï¼Œå°è¯•å•æ¡ä¿å­˜ï¼ˆé™çº§ç­–ç•¥ï¼‰
            for metrics in metrics_list:
                try:
                    await self.data_persistence.save_metrics(metrics)
                except Exception as single_error:
                    self.logger.error(f"âŒ å•æ¡ä¿å­˜æŒ‡æ ‡æ•°æ®å¤±è´¥: monitor_id={metrics.monitor_id}, error={single_error}")
    
    async def flush_pending_metrics(self):
        """åˆ·æ–°å¾…ä¿å­˜çš„æŒ‡æ ‡æ•°æ®ï¼ˆç”¨äºæœåŠ¡å…³é—­æ—¶è°ƒç”¨ï¼‰"""
        await self._flush_batch_queue()
    
    async def _save_metrics_async(self, metrics):
        """å¼‚æ­¥ä¿å­˜æŒ‡æ ‡æ•°æ®ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰- ä¿ç•™ç”¨äºå…¼å®¹æ€§"""
        try:
            await self._ensure_db_initialized()
            await self.data_persistence.save_metrics(metrics)
            self.logger.debug(f"âœ… æŒ‡æ ‡æ•°æ®ä¿å­˜æˆåŠŸ: monitor_id={metrics.monitor_id}")
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æŒ‡æ ‡æ•°æ®å¤±è´¥: {e}", exc_info=True) 