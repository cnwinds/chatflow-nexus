"""
Agent èŠ‚ç‚¹

è¾“å…¥:
- user_text: ç”¨æˆ·æ–‡æœ¬
- confidence: ç½®ä¿¡åº¦
- tts_current_sentence: TTS å½“å‰æ’­æ”¾å¥å­ï¼ˆåé¦ˆå›è·¯ï¼‰
- tts_all_complete: TTS å…¨éƒ¨å®Œæˆï¼ˆåé¦ˆå›è·¯ï¼‰

è¾“å‡º:
- response_text_stream: æ–‡æœ¬å¢é‡ï¼ˆæµï¼Œç©ºæ–‡æœ¬è¡¨ç¤ºç»“æŸï¼‰
"""

import sys
from pathlib import Path
from typing import Any, Dict, Optional
import time
import json
# ç¡®ä¿å¯ä»¥å¯¼å…¥ src æ¨¡å—ï¼ˆå½“ä»å¤–éƒ¨é¡¹ç›®åŠ è½½æ—¶ï¼‰
_file_path = Path(__file__).resolve()
_project_root = _file_path.parent.parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

from stream_workflow.core.parameter import FieldSchema
from stream_workflow.core import Node, ParameterSchema, StreamChunk, register_node

from src.common.utils.llm_chat import LLMChat
from src.common.utils.date_utils import get_current_time, get_lunar_date_str
from src.common.logging import get_logger
from datetime import datetime

logger = get_logger(__name__)


@register_node("agent_node")
class AgentNode(Node):
    """è´Ÿè´£ä¸ AI è¿›è¡Œå¯¹è¯ï¼Œå¤„ç†ç”¨æˆ·æ–‡æœ¬å¹¶ç”Ÿæˆå›å¤ã€‚
    
    åŠŸèƒ½: æ¥æ”¶ç”¨æˆ·æ–‡æœ¬è¾“å…¥ï¼Œè°ƒç”¨ LLM ç”Ÿæˆå›å¤ï¼Œæ”¯æŒæµå¼è¾“å‡ºã€‚å¯ä»¥å¤„ç†æ™®é€šå¯¹è¯ã€è·¯ç”±æŒ‡ä»¤å’Œè‡ªæˆ‘ä»‹ç»è¯·æ±‚ã€‚
    æ”¯æŒå¤š agent åä½œï¼Œå¯ä»¥æ¥æ”¶å…¶ä»– agent çš„ä»‹ç»ä¿¡æ¯ï¼Œå¹¶åœ¨ç”Ÿæˆå›å¤æ—¶è€ƒè™‘å¯ç”¨çš„ agentã€‚
    
    é…ç½®å‚æ•°:
    - system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼Œç”¨äºå®šä¹‰ agent çš„è§’è‰²å’Œè¡Œä¸ºã€‚æ”¯æŒ Jinja2 æ¨¡æ¿è¯­æ³•ï¼Œå¯ä½¿ç”¨å˜é‡å¦‚ agent_idã€agent_introã€user_message ç­‰ã€‚
    - user_prompt: ç”¨æˆ·æç¤ºè¯ï¼Œç”¨äºæ ¼å¼åŒ–ç”¨æˆ·è¾“å…¥ã€‚æ”¯æŒ Jinja2 æ¨¡æ¿è¯­æ³•ï¼Œå¯ä½¿ç”¨å˜é‡å¦‚ user_messageã€confidenceã€emotion ç­‰ã€‚
    - intro: agent ä»‹ç»æ–‡æœ¬ï¼Œå½“æ”¶åˆ°è‡ªæˆ‘ä»‹ç»è¯·æ±‚æ—¶ä¼šå‘é€ç»™å…¶ä»– agentã€‚æ”¯æŒ Jinja2 æ¨¡æ¿è¯­æ³•ï¼Œå¯ä½¿ç”¨å˜é‡å¦‚ agent_idã€‚
    """
    
    EXECUTION_MODE = "streaming"    # è¾“å…¥å‚æ•°å®šä¹‰
    INPUT_PARAMS = {
        "user_text": ParameterSchema(
            is_streaming=True,
            schema={'text': 'string', 'confidence': 'float', 'audio_file_path': 'string', 'emotion': 'string'}
        ),
        "tts_status": ParameterSchema(
            is_streaming=True,
            schema={'state': 'string', 'text': 'string'}
        ),
        "intro_request": ParameterSchema(
            is_streaming=True,
            schema={}
        ),
        "all_agents_intro": ParameterSchema(
            is_streaming=True,
            schema={'agents': 'object'}
        ),
        "route_text": ParameterSchema(
            is_streaming=True,
            schema={'user_query': 'string', 'transition_text': 'string'}
        )
    }    # è¾“å‡ºå‚æ•°å®šä¹‰
    OUTPUT_PARAMS = {
        "response_text_stream": ParameterSchema(
            is_streaming=True,
            schema={'text': 'string'}
        ),
        "agent_intro": ParameterSchema(
            is_streaming=True,
            schema={'agent_id': 'string', 'intro_text': 'string'}
        )
    }    # é…ç½®å‚æ•°å®šä¹‰ï¼ˆä½¿ç”¨ FieldSchema æ ¼å¼ï¼‰
    CONFIG_PARAMS = {
        "system_prompt": FieldSchema({
            'type': 'string',
            'required': True,
            'description': 'ç³»ç»Ÿæç¤ºè¯'
        }),
        "user_prompt": FieldSchema({
            'type': 'string',
            'required': True,
            'description': 'ç”¨æˆ·æç¤ºè¯'
        }),
        "intro": FieldSchema({
            'type': 'string',
            'required': True,
            'description': 'agentä»‹ç»'
        })
    }

    async def initialize(self, context):
        """åˆå§‹åŒ–èŠ‚ç‚¹ - åœ¨runä¹‹å‰è°ƒç”¨ï¼Œç¡®ä¿æ‰€æœ‰èµ„æºåœ¨æ¥æ”¶æ•°æ®å‰å·²å‡†å¤‡å¥½"""
        self.context = context

        # ä»å…¨å±€ä¸Šä¸‹æ–‡è·å–é…ç½®
        self.agent_id = context.get_global_var("agent_id")
        self.session_id = context.get_global_var("session_id")
        self.engine = context.get_global_var("engine")
        self.user_data = context.get_global_var("user_data")

        # ä»èŠ‚ç‚¹é…ç½®è·å– agent ç‰¹å®šé…ç½®
        self.intro = self.get_config("config.intro")
        self.system_prompt = self.get_config("config.system_prompt")
        self.user_prompt = self.get_config("config.user_prompt")
        
        # éªŒè¯å¿…éœ€é…ç½®
        if not self.system_prompt or not self.user_prompt:
            self.context.log_error(f"Agent {self.node_id} é…ç½®é”™è¯¯ï¼šå¿…é¡»æä¾› config.system_prompt å’Œ config.user_prompt")
            return
        
        # å­˜å‚¨å…¶ä»– agent çš„ä»‹ç»ä¿¡æ¯
        self.available_agents = {}

        # åŠ è½½é…ç½®
        ai_providers = context.get_global_var("ai_providers") or {}
        self.llm_config = ai_providers.get("llm", {})
        
        # 1. åˆå§‹åŒ– LLMï¼ˆä¼šè‡ªåŠ¨åŠ è½½åŸºç¡€é…ç½®ï¼‰
        self.llm: Optional[LLMChat] = LLMChat()
        self.llm.load_config(self.llm_config)

        # 2. åˆ›å»ºèŠå¤©è®°å½•ç®¡ç†å™¨
        self.chat_record = context.get_global_var("chat_record_node")
        
        # 3. è·å–å·¥å…·åˆ—è¡¨ï¼ˆå…¨å±€ UTCPï¼‰
        from src.agents.utcp_tools import get_utcp_tools
        self.tools = await get_utcp_tools(tags=["llm_tools"])

        self._is_playing: bool = False

    async def run(self, context):
        """è¿è¡ŒèŠ‚ç‚¹ - æŒç»­è¿è¡Œï¼Œç­‰å¾…å¤„ç†æµå¼æ•°æ®"""
        import asyncio
        await asyncio.sleep(float("inf"))

    async def on_chunk_received(self, param_name: str, chunk: StreamChunk):
        # æ’­æ”¾åé¦ˆï¼šæ›´æ–°å†…éƒ¨æ’­æ”¾çŠ¶æ€
        if param_name == "tts_status":
            data = chunk.data or {}
            state = data.get("state", "")
            if state == "start":
                self._is_playing = True
            elif state == "stop":
                self._is_playing = False
            return

        # å¤„ç†è‡ªæˆ‘ä»‹ç»è¯·æ±‚
        if param_name == "intro_request":
            await self._handle_intro_request()
            return
        
        # å¤„ç†å…¶ä»– agent ä»‹ç»
        if param_name == "all_agents_intro":
            await self._handle_all_agents_intro(chunk)
            return

        # å¤„ç†è·¯ç”±æ–‡æœ¬
        if param_name == "route_text":
            await self._handle_route_text(chunk)
            return

        if param_name == "user_text":
            text = (chunk.data or {}).get("text", "")
            confidence = (chunk.data or {}).get("confidence", None)
            audio_file_path = (chunk.data or {}).get("audio_file_path", None)
            emotion = (chunk.data or {}).get("emotion", None)

            if not text:
                await self.emit_chunk("response_text_stream", {"text": ""})
                return
            
            # å‡†å¤‡æ ¼å¼åŒ–å˜é‡å¹¶è°ƒç”¨ LLM
            format_vars = self._prepare_format_vars(
                user_message=text,
                confidence=confidence,
                emotion=emotion
            )
            
            await self._call_llm_and_stream(format_vars, context_name=f"{self.node_id} æµå¼å¯¹è¯")

    def _prepare_format_vars(self, user_message: str, confidence: Optional[float] = None,
                            emotion: Optional[str] = None, transition_text: Optional[str] = None,
                            user_query: Optional[str] = None) -> Dict[str, Any]:
        """å‡†å¤‡æ ¼å¼åŒ–å˜é‡ï¼ˆå…¬å…±æ–¹æ³•ï¼‰
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯æ–‡æœ¬
            confidence: ç½®ä¿¡åº¦ï¼ˆå¯é€‰ï¼‰
            emotion: ç”¨æˆ·æƒ…æ„Ÿï¼ˆå¯é€‰ï¼‰
            transition_text: è½¬åœºæ–‡æœ¬ï¼ˆè·¯ç”±åœºæ™¯ä½¿ç”¨ï¼‰
            user_query: ç”¨æˆ·åŸå§‹éœ€æ±‚ï¼ˆè·¯ç”±åœºæ™¯ä½¿ç”¨ï¼‰
            
        Returns:
            æ ¼å¼åŒ–å˜é‡å­—å…¸
        """
        # è·å–å…¬å…±æ•°æ®
        voice_name = self.user_data.get_memory("preferences.current_voice") or "original"
        char_prompt = self.user_data.get_config("profile.character.prompt") or ""
        long_term_memory = self.user_data.get_memory("chat.long_term_memory") or None
        
        
        # è·å–æ¨¡å¼æŒ‡ç¤ºå™¨
        # åˆ¤æ–­ä½ç½®ä¿¡åº¦æ¨¡å¼ï¼ˆæ ¹æ®ç½®ä¿¡åº¦é˜ˆå€¼åˆ¤æ–­ï¼‰
        raw_thresholds = self.user_data.get_config("audio_settings.confidence_threshold")
        default_thresholds = [0.8, 0.5]
        
        # åˆ¤æ–­è¾“å…¥æ˜¯å¦æ»¡è¶³è¦æ±‚ï¼šå¿…é¡»æ˜¯åŒ…å«è‡³å°‘2ä¸ªæ•°å­—çš„åˆ—è¡¨/å…ƒç»„
        if (isinstance(raw_thresholds, (list, tuple)) and 
            len(raw_thresholds) >= 2 and 
            all(isinstance(x, (int, float)) for x in raw_thresholds[:2])):
            confidence_thresholds = [float(raw_thresholds[0]), float(raw_thresholds[1])]
        else:
            confidence_thresholds = default_thresholds
            logger.warning(f"ç½®ä¿¡åº¦é˜ˆå€¼é…ç½®æ— æ•ˆ: {raw_thresholds}ï¼Œä½¿ç”¨é»˜è®¤å€¼: {default_thresholds}")
        
        threshold2 = confidence_thresholds[1]
        is_low_confidence = confidence is not None and confidence < threshold2
        
        # ä½ç½®ä¿¡åº¦æƒ…å†µä¸‹ï¼Œæ ¹æ®æ˜¯å¦å¼€å¯å‘€å‘€å­¦è¯­æ¨¡å¼å†³å®šæ˜¾ç¤ºå“ªä¸ªæ¨¡å¼
        if is_low_confidence:
            enable_baby_talk_mode = self.user_data.get_config("audio_settings.enable_baby_talk_mode") or False
            if enable_baby_talk_mode:
                mode_indicator = "ğŸµ"
            else:
                mode_indicator = "âš ï¸"
        else:
            mode_indicator = ""
        
        # è·å–å¯ç”¨å£°éŸ³åˆ—è¡¨
        available_voices = self.user_data.get_config("clone_voice._voice_names") or []
        
        # è·å–å¼•å¯¼è¯é¢˜å’Œç­–ç•¥ï¼ˆä» memory ä¸­è·å–ï¼‰
        guidance_topic = self.user_data.get_config("guidance.topic") or None
        guidance_strategy = self.user_data.get_config("guidance.strategy") or None
        
        now = datetime.now()
        format_vars = {
            "character_prompt": char_prompt,

            "current_time": get_current_time(now),  # å…¬å†æ—¶é—´
            "lunar_date": get_lunar_date_str(now),  # å†œå†æ—¥æœŸï¼ˆå¯é€‰ï¼‰
            "weekday": time.strftime("%A"),
            "mode_indicator": mode_indicator,
            "voice_name": voice_name,

            "confidence": confidence if confidence is not None else 1.0,
            "user_emotion": emotion or "neutral",
            "user_message": user_message,

            "long_term_memory": long_term_memory,  # æ¨¡æ¿ä¸­ä½¿ç”¨ tojson è¿‡æ»¤å™¨

            "available_agents": self.available_agents,
            "current_agent_id": self.node_id,
            "current_agent_intro": self.intro or "",  # å½“å‰ä¼™ä¼´ä¸“é•¿

            "available_voices": available_voices,

            "guidance_topic": guidance_topic,
            "guidance_strategy": guidance_strategy,
            
            "has_transition": transition_text is not None,
            "transition_text": transition_text,
        }
        
        return format_vars
    
    async def _call_llm_and_stream(self, format_vars: Dict[str, Any], context_name: str):
        """è°ƒç”¨ LLM å¹¶å¤„ç†æµå¼å“åº”ï¼ˆä½¿ç”¨ LLMChat å·¥å…·ç±»ï¼‰
        
        Args:
            format_vars: æ ¼å¼åŒ–å˜é‡å­—å…¸
            context_name: ä¸Šä¸‹æ–‡åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        """
        # æ¸²æŸ“æç¤ºè¯æ¨¡æ¿
        system_prompt_text = self.engine.render_template(self.system_prompt, **format_vars)
        user_prompt_text = self.engine.render_template(self.user_prompt, **format_vars)

        # è·å–æ¶ˆæ¯åˆ—è¡¨ï¼ˆchat_record_node å·²è‡ªåŠ¨æ·»åŠ ä¸Šä¸‹æ–‡ï¼‰
        if self.chat_record is None:
            self.context.log_warning(f"Agent {self.node_id} chat_record_node æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç©ºå†å²è®°å½•")
            # æ„å»ºåŸºæœ¬æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ— å†å²è®°å½•ï¼‰
            messages = []
            if system_prompt_text and system_prompt_text.strip():
                messages.append({"role": "system", "content": system_prompt_text.strip()})
            if user_prompt_text and user_prompt_text.strip():
                messages.append({"role": "user", "content": user_prompt_text.strip()})
        else:
            # ç­‰å¾…å†å²è®°å½•åŠ è½½å®Œæˆï¼ˆå¦‚æœæ­£åœ¨åŠ è½½ï¼‰
            await self.chat_record.wait_for_history_loaded()
            messages = self.chat_record.get_chat_messages(system_prompt_text, user_prompt_text)
        
        # æµå¼å“åº”å›è°ƒï¼šå°†å†…å®¹å¢é‡å‘é€åˆ°è¾“å‡ºæµ
        async def on_delta(chunk_type: str, data: Dict[str, Any]):
            if chunk_type == "content_delta":
                delta = (data.get("delta") or {}).get("content", "")
                if delta:
                    await self.emit_chunk("response_text_stream", {"text": delta})
        
        # ä½¿ç”¨ LLMChat å·¥å…·ç±»è¿›è¡Œæµå¼è°ƒç”¨
        try:
            await self.llm.call_llm_stream(
                messages=messages,
                tools=self.tools,
                context=context_name,
                content_callback=on_delta,
                session_id=self.session_id,
                model="primary"
            )
        except Exception as e:
            self.context.log_error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            raise
        finally:
            # å‘é€ç©ºæ–‡æœ¬è¡¨ç¤ºæµç»“æŸ
            await self.emit_chunk("response_text_stream", {"text": ""})
    
    async def _handle_intro_request(self):
        """å¤„ç†è‡ªæˆ‘ä»‹ç»è¯·æ±‚"""
        try:
            if not self.intro:
                self.context.log_error(f"Agent {self.node_id} é…ç½®é”™è¯¯ï¼šå¿…é¡»æä¾› config.intro")
                return
            
            # ä½¿ç”¨ engine æä¾›çš„ Jinja2 æ¨¡æ¿æ¸²æŸ“è‡ªæˆ‘ä»‹ç»æ–‡æœ¬
            format_vars = {"agent_id": self.node_id}
            intro_text = self.engine.render_template(self.intro, **format_vars)
            
            # å‘é€è‡ªæˆ‘ä»‹ç»
            await self.emit_chunk("agent_intro", {
                "agent_id": self.node_id,
                "intro_text": intro_text
            })
            
            self.context.log_info(f"Agent {self.node_id} å‘é€è‡ªæˆ‘ä»‹ç»: {intro_text}")
            
        except Exception as e:
            self.context.log_error(f"Agent {self.node_id} å¤„ç†è‡ªæˆ‘ä»‹ç»è¯·æ±‚å¤±è´¥: {e}")

    async def _handle_all_agents_intro(self, chunk: StreamChunk):
        """å¤„ç†æ‰€æœ‰ agent ä»‹ç»"""
        try:
            data = chunk.data or {}
            agents = data.get("agents", {})
            
            # è¿‡æ»¤æ‰è‡ªå·±çš„ä»‹ç»
            self.available_agents = {k: v for k, v in agents.items() if k != self.node_id}
            
            self.context.log_info(f"Agent {self.node_id} æ”¶åˆ°å…¶ä»– agent ä»‹ç»: {self.available_agents}")
            
        except Exception as e:
            self.context.log_error(f"Agent {self.node_id} å¤„ç†å…¶ä»– agent ä»‹ç»å¤±è´¥: {e}")

    async def _handle_route_text(self, chunk: StreamChunk):
        """å¤„ç†è·¯ç”±æ–‡æœ¬ï¼ŒåŸºäºç”¨æˆ·é—®é¢˜å’Œå·²æœ‰è½¬åœºå†…å®¹ç»§ç»­ç”Ÿæˆ"""
        try:
            data = chunk.data or {}
            user_query = data.get("user_query", "")
            transition_text = data.get("transition_text", "")

            # å‡†å¤‡æ ¼å¼åŒ–å˜é‡å¹¶è°ƒç”¨ LLM
            format_vars = self._prepare_format_vars(
                user_message=user_query,
                transition_text=transition_text,
                user_query=user_query
            )
            
            await self._call_llm_and_stream(format_vars, context_name=f"{self.node_id} è·¯ç”±ç»§ç»­ç”Ÿæˆ")
            
        except Exception as e:
            self.context.log_error(f"Agent {self.node_id} å¤„ç†è·¯ç”±æ–‡æœ¬å¤±è´¥: {e}")
            await self.emit_chunk("response_text_stream", {"text": ""})
    

