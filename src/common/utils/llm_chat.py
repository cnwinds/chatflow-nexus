#!/usr/bin/env python3
"""
LLMèŠå¤©æ¨¡å— - é«˜å†…èšçš„LLMè°ƒç”¨å°è£…

æä¾›ç»Ÿä¸€çš„LLMèŠå¤©åŠŸèƒ½ï¼ŒåŒ…æ‹¬é˜»å¡è°ƒç”¨å’Œæµå¼è°ƒç”¨ï¼Œ
è‡ªåŠ¨ç®¡ç†èŠå¤©æ¶ˆæ¯ï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨è¿‡ç¨‹ä¸­çš„æ¶ˆæ¯ã€‚
"""

import json
import time
import logging
from typing import Dict, Any, List, Optional, AsyncGenerator
from dataclasses import dataclass

from src.common.config import get_config_manager
from src.common.logging.manager import LoggingManager
from src.common.config.constants import ConfigPaths
from src.utcp.streaming import StreamResponse
from src.agents.utcp_tools import call_utcp_tool, call_utcp_tool_stream


class LLMResponseError(Exception):
    """LLMå“åº”å¤„ç†ç›¸å…³å¼‚å¸¸"""
    pass


class ToolCallValidationError(LLMResponseError):
    """å·¥å…·è°ƒç”¨æ•°æ®éªŒè¯å¼‚å¸¸"""
    pass


@dataclass
class ToolCall:
    """å·¥å…·è°ƒç”¨æ•°æ®ç»“æ„"""
    id: str
    type: str = "function"
    function_name: str = ""
    function_arguments: str = "{}"
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ToolCall':
        """ä»å­—å…¸åˆ›å»ºToolCallå®ä¾‹"""
        if not isinstance(data, dict):
            raise ToolCallValidationError(f"å·¥å…·è°ƒç”¨æ•°æ®å¿…é¡»æ˜¯å­—å…¸ç±»å‹ï¼Œå®é™…ç±»å‹: {type(data)}")
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if not data.get("id"):
            raise ToolCallValidationError("å·¥å…·è°ƒç”¨ç¼ºå°‘å¿…éœ€çš„ 'id' å­—æ®µ")
        
        function_data = data.get("function", {})
        if not isinstance(function_data, dict):
            raise ToolCallValidationError("å·¥å…·è°ƒç”¨çš„ 'function' å­—æ®µå¿…é¡»æ˜¯å­—å…¸ç±»å‹")
        
        # éªŒè¯ function_arguments æ˜¯å¦ä¸ºæœ‰æ•ˆ JSON
        arguments = function_data.get("arguments", "{}")
        try:
            json.loads(arguments)
        except json.JSONDecodeError as e:
            raise ToolCallValidationError(f"å·¥å…·è°ƒç”¨å‚æ•°ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼: {e}")
        
        return cls(
            id=data["id"],
            type=data.get("type", "function"),
            function_name=function_data.get("name", ""),
            function_arguments=arguments
        )
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "id": self.id,
            "type": self.type,
            "function": {
                "name": self.function_name,
                "arguments": self.function_arguments
            }
        }


@dataclass
class Usage:
    """Tokenä½¿ç”¨ç»Ÿè®¡"""
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    
    @classmethod
    def from_dict(cls, data: Optional[Dict[str, Any]]) -> 'Usage':
        """ä»å­—å…¸åˆ›å»ºUsageå®ä¾‹"""
        if not data:
            return cls()
        return cls(
            prompt_tokens=data.get('prompt_tokens', 0),
            completion_tokens=data.get('completion_tokens', 0),
            total_tokens=data.get('total_tokens', 0)
        )


@dataclass
class LLMResponse:
    """æ ‡å‡†åŒ–çš„LLMå“åº”æ•°æ®ç»“æ„"""
    content: str
    tool_calls: List[ToolCall]
    usage: Usage
    used_model: str
    request_model: str

    @classmethod
    def create(cls, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None, 
               usage: Optional[Dict[str, Any]] = None, used_model: str = None, 
               request_model: str = None) -> 'LLMResponse':
        """åˆ›å»ºLLMResponseå®ä¾‹"""
        parsed_tool_calls = []
        if tool_calls:
            try:
                parsed_tool_calls = [ToolCall.from_dict(tc) for tc in tool_calls]
            except (ToolCallValidationError, TypeError, AttributeError) as e:
                raise LLMResponseError(f"è§£æå·¥å…·è°ƒç”¨æ•°æ®å¤±è´¥: {e}") from e
        
        return cls(
            content=content or "",
            tool_calls=parsed_tool_calls,
            usage=Usage.from_dict(usage),
            used_model=used_model,
            request_model=request_model
        )
    
    def has_tool_calls(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨"""
        return len(self.tool_calls) > 0
    
    def get_tool_calls_dict(self) -> List[Dict[str, Any]]:
        """è·å–å·¥å…·è°ƒç”¨çš„å­—å…¸æ ¼å¼"""
        return [tc.to_dict() for tc in self.tool_calls]
    
    def get_used_model(self) -> str:
        """è·å–ä½¿ç”¨çš„æ¨¡å‹"""
        return self.used_model
    
    def get_request_model(self) -> str:
        """è·å–è¯·æ±‚çš„æ¨¡å‹"""
        return self.request_model


class LLMChat:
    """LLMèŠå¤©æ¨¡å—"""
    
    # å¸¸é‡å®šä¹‰
    DEFAULT_MAX_TOKENS = 1500
    DEFAULT_TEMPERATURE = 1.0
    DEFAULT_TOP_P = 1.0
    DEFAULT_MAX_ITERATIONS = 10
    
    def __init__(self):
        # ä½¿ç”¨æ ¸å¿ƒç»„ä»¶
        self.config_manager = get_config_manager()
        self.logging_manager = LoggingManager(self.config_manager)
        self.logger = self.logging_manager.get_logger("llm_chat")
        
        # åŸå§‹é…ç½®
        self.llm_config = None
        self._config_loaded = False
        
        # é¢„è§£æçš„é…ç½®ï¼ˆæå‰è§£æï¼Œé¿å…æ¯æ¬¡è°ƒç”¨æ—¶é‡å¤è§£æï¼‰
        self._parsed_configs = {}
        self._default_model_key = None
        
        # æ¶ˆæ¯ç®¡ç†
        self.conversation_history: List[Dict[str, Any]] = []
    
    def load_config(self, config: Dict[str, Any] = None):
        """
        åŠ è½½å¹¶è§£æLLMé…ç½®
        
        Args:
            config: LLMé…ç½®å­—å…¸
        """
        if not self._config_loaded:
            # ä¿å­˜åŸå§‹é…ç½®
            self.llm_config = config or {"primary": "azure_llm.primary", "fast": "ollama_llm.fast"}
            
            # æå‰è§£ææ‰€æœ‰é…ç½®
            self._parse_all_configs()
            
            self._config_loaded = True
    
    def _parse_all_configs(self):
        """æå‰è§£ææ‰€æœ‰é…ç½®é¡¹ï¼Œé¿å…è¿è¡Œæ—¶é‡å¤è§£æ"""
        # è§£ææ¯ä¸ªé…ç½®é¡¹
        for key, provider_model in self.llm_config.items():
            if "." in provider_model:
                service_name, model_name = provider_model.split(".", 1)
            else:
                service_name = provider_model
                model_name = key
            
            provider = service_name.split("_")[0]
            
            # æå‰æ‹¼æ¥å¥½å®Œæ•´çš„æœåŠ¡åï¼Œé¿å…è¿è¡Œæ—¶æ‹¼æ¥
            self._parsed_configs[key] = {
                "provider": provider,
                "service_name": service_name + ".chat_completion",
                "stream_service_name": service_name + ".chat_completion_stream",
                "model_name": model_name
            }
        
        # è®¾ç½®é»˜è®¤æ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨primaryï¼Œå¦åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªï¼‰
        if "primary" in self._parsed_configs:
            self._default_model_key = "primary"
        else:
            self._default_model_key = next(iter(self._parsed_configs)) if self._parsed_configs else None
    
    def _get_service_names(self, model: Optional[str] = None) -> tuple[str, str, str, str]:
        """
        æ ¹æ®modelè·å–æœåŠ¡åï¼ˆç›´æ¥è¿”å›é¢„è§£æçš„é…ç½®ï¼‰
        
        Args:
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œå¦‚ "primary", "fast"ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            
        Returns:
            (provider, service_name, stream_service_name, model_name)
        """
        # ç¡®å®šä½¿ç”¨å“ªä¸ªæ¨¡å‹é…ç½®
        model_key = model if model else self._default_model_key
        # ä»é¢„è§£æçš„é…ç½®ä¸­è·å–
        if model_key and model_key in self._parsed_configs:
            parsed = self._parsed_configs[model_key]
        elif self._default_model_key and self._default_model_key in self._parsed_configs:
            # å¦‚æœæŒ‡å®šçš„æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
            parsed = self._parsed_configs[self._default_model_key]
            self.logger.warning(f"æ¨¡å‹ {model_key} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹ {self._default_model_key}")
        else:
            # ä½¿ç”¨ç¡¬ç¼–ç çš„é»˜è®¤å€¼
            self.logger.warning("æ²¡æœ‰å¯ç”¨çš„é…ç½®ï¼Œä½¿ç”¨ç¡¬ç¼–ç é»˜è®¤å€¼")
            return (
                "azure",
                "azure_llm.chat_completion",
                "azure_llm.chat_completion_stream",
                "primary"
            )
        
        # ç›´æ¥è¿”å›é¢„è§£æå¥½çš„é…ç½®ï¼Œæ— éœ€è¿è¡Œæ—¶æ‹¼æ¥
        return (
            parsed["provider"],
            parsed["service_name"],
            parsed["stream_service_name"],
            parsed["model_name"]
        )
    
    # å…¬å…±æ¥å£æ–¹æ³•
    async def call_llm(self, messages: List[Dict[str, Any]], 
                      tools: Optional[List[Dict[str, Any]]] = None,
                      max_iterations: int = DEFAULT_MAX_ITERATIONS,
                      max_tokens: int = DEFAULT_MAX_TOKENS,
                      temperature: float = DEFAULT_TEMPERATURE,
                      top_p: float = DEFAULT_TOP_P,
                      context: str = "èŠå¤©å¯¹è¯",
                      session_id: str = None,
                      model: Optional[str] = None) -> LLMResponse:
        """
        ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£ï¼ˆé˜»å¡è°ƒç”¨ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: å·¥å…·åˆ—è¡¨
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: top_på‚æ•°
            context: ä¸Šä¸‹æ–‡æè¿°
            model: æŒ‡å®šæ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸ºNone
            
        Returns:
            LLMResponse: å®Œæ•´çš„LLMå“åº”å¯¹è±¡
        """
        conversation_history = messages.copy()
        iteration_count = 0
        
        while iteration_count < max_iterations:
            try:
                # è°ƒç”¨LLM API
                response = await self._call_llm_api(conversation_history, tools, 
                                                  max_tokens, temperature, top_p, context, session_id, model)
                
                if not response:
                    raise ValueError("Invalid API response")
                
                if response.has_tool_calls():
                    # å¤„ç†å·¥å…·è°ƒç”¨
                    tool_calls_data = response.get_tool_calls_dict()
                    
                    conversation_history.append({
                        "role": "assistant",
                        "content": response.content or "",
                        "tool_calls": tool_calls_data
                    })
                    
                    await self._process_tool_calls(response.tool_calls, conversation_history)
                    iteration_count += 1
                    continue
                else:
                    # æ— å·¥å…·è°ƒç”¨ï¼Œè¿”å›å“åº”
                    content = response.content or ""
                    conversation_history.append({
                        "role": "assistant",
                        "content": content
                    })
                    return response
                    
            except Exception as e:
                self.logger.error(f"Error processing chat with tools: {e}")
                raise
        
        self.logger.warning(f"Reached max iterations ({max_iterations})")
        error_response = LLMResponse.create("æŠ±æ­‰ï¼Œå·¥å…·è°ƒç”¨æ¬¡æ•°è¿‡å¤šï¼Œè¯·é‡æ–°å¼€å§‹å¯¹è¯ã€‚")
        return error_response
    
    async def call_llm_stream(self, messages: List[Dict[str, Any]], 
                            tools: Optional[List[Dict[str, Any]]] = None,
                            max_iterations: int = DEFAULT_MAX_ITERATIONS,
                            max_tokens: int = DEFAULT_MAX_TOKENS,
                            temperature: float = DEFAULT_TEMPERATURE,
                            top_p: float = DEFAULT_TOP_P,
                            context: str = "æµå¼èŠå¤©",
                            session_id: str = None,
                            model: Optional[str] = None,
                            content_callback: Optional[callable] = None) -> LLMResponse:
        """
        æµå¼LLMè°ƒç”¨æ¥å£ï¼ˆé€šè¿‡å›è°ƒå¤„ç†æµå¼å†…å®¹ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: å·¥å…·åˆ—è¡¨
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: top_på‚æ•°
            context: ä¸Šä¸‹æ–‡æè¿°
            model: æŒ‡å®šæ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸ºNone
            content_callback: å†…å®¹å›è°ƒå‡½æ•°ï¼Œç”¨äºå®æ—¶å¤„ç†æµå¼å†…å®¹ï¼Œæ¥æ”¶å‚æ•°(chunk_type, chunk_data)
            
        Returns:
            LLMResponse: æœ€ç»ˆçš„å®Œæ•´å“åº”
        """
        conversation_history = messages.copy()
        iteration_count = 0
        
        while iteration_count < max_iterations:
            try:
                # ä½¿ç”¨æµå¼APIè°ƒç”¨ï¼Œæ”¯æŒå›è°ƒå‡½æ•°
                response = await self._call_llm_stream_api(
                    conversation_history, tools, max_tokens, temperature, 
                    top_p, context, session_id, model, content_callback)
                
                if not response:
                    raise ValueError("Invalid API response")
                
                if response.has_tool_calls():
                    # å¤„ç†å·¥å…·è°ƒç”¨
                    tool_calls_data = response.get_tool_calls_dict()
                    
                    conversation_history.append({
                        "role": "assistant",
                        "content": response.content or "",
                        "tool_calls": tool_calls_data
                    })
                    
                    await self._process_tool_calls(response.tool_calls, conversation_history)
                    iteration_count += 1
                    continue
                else:
                    # æ— å·¥å…·è°ƒç”¨ï¼Œè¿”å›å“åº”
                    content = response.content or ""
                    conversation_history.append({
                        "role": "assistant",
                        "content": content
                    })
                    return response
                    
            except Exception as e:
                self.logger.error(f"Error processing stream chat with tools: {e}")
                raise
        
        self.logger.warning(f"Reached max iterations ({max_iterations})")
        # è¿”å›é”™è¯¯å“åº”
        error_response = LLMResponse.create("æŠ±æ­‰ï¼Œå·¥å…·è°ƒç”¨æ¬¡æ•°è¿‡å¤šï¼Œè¯·é‡æ–°å¼€å§‹å¯¹è¯ã€‚")
        return error_response
    
    # æ¶ˆæ¯ç®¡ç†æ–¹æ³•
    def add_message(self, role: str, content: str = None, **kwargs) -> None:
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²"""
        # å¦‚æœcontentåœ¨kwargsä¸­ï¼Œä¼˜å…ˆä½¿ç”¨kwargsä¸­çš„content
        if content is None and "content" in kwargs:
            content = kwargs.pop("content")
        
        message = {"role": role, "content": content}
        message.update(kwargs)
        self.conversation_history.append(message)
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """è·å–å¯¹è¯å†å²"""
        return self.conversation_history.copy()
    
    def clear_conversation(self) -> None:
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history = []
    
    # ç§æœ‰æ–¹æ³•ï¼ˆå®ç°ç»†èŠ‚ï¼‰
    async def _call_llm_api(self, messages: List[Dict[str, Any]], 
                           tools: Optional[List[Dict[str, Any]]] = None,
                           max_tokens: int = DEFAULT_MAX_TOKENS,
                           temperature: float = DEFAULT_TEMPERATURE,
                           top_p: float = DEFAULT_TOP_P,
                           context: str = "èŠå¤©", 
                           session_id: str = None,
                           model: Optional[str] = None) -> LLMResponse:
        """è°ƒç”¨LLM APIï¼ˆé˜»å¡ç‰ˆæœ¬ï¼‰"""
        start_time = time.time()
        
        try:
            # å¼€å§‹AIæŒ‡æ ‡ç›‘æ§
            monitor_id = None
            try:
                result = await call_utcp_tool("ai_metrics_service.start_monitoring", {})
                monitor_id = result.get("monitor_id")
            except Exception as e:
                self.logger.debug(f"å¯åŠ¨AIæŒ‡æ ‡ç›‘æ§å¤±è´¥: {e}")

            self._log_request_details(messages, context, tools)
            
            # åŠ¨æ€è·å–æœåŠ¡åç§°ï¼ˆä½¿ç”¨é¢„è§£æçš„é…ç½®ï¼‰
            provider, service_name, stream_service_name, model_name = self._get_service_names(model)
            # å‡†å¤‡APIå‚æ•°
            api_params = self._prepare_api_params(messages, tools, max_tokens, temperature, top_p, model_name)
            # è°ƒç”¨API
            result = await call_utcp_tool(service_name, api_params)
            
            # å®Œæˆç›‘æ§
            if monitor_id:
                try:
                    await call_utcp_tool("ai_metrics_service.finish_monitoring", {
                        "monitor_id": monitor_id,
                        "provider": provider,
                        "model_name": result.get("used_model"),
                        "session_id": session_id,
                        "prompt_tokens": result.get("usage", {}).get("prompt_tokens", 0),
                        "completion_tokens": result.get("usage", {}).get("completion_tokens", 0)
                    })
                except Exception as e:
                    self.logger.debug(f"å®ŒæˆAIæŒ‡æ ‡ç›‘æ§å¤±è´¥: {e}")
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            process_duration = (time.time() - start_time) * 1000
            return await self._handle_llm_response(result, context, process_duration)
            
        except Exception as e:
            self.logger.error(f"âŒ {context}APIè°ƒç”¨å¤±è´¥: {e}")
            if monitor_id:
                try:
                    await call_utcp_tool("ai_metrics_service.finish_monitoring", {
                        "monitor_id": monitor_id,
                        "provider": provider,
                        "result": str(e)
                    })
                except Exception:
                    pass
            raise
    
    async def _call_llm_stream_api(self, messages: List[Dict[str, Any]], 
                                                tools: Optional[List[Dict[str, Any]]] = None,
                                                max_tokens: int = DEFAULT_MAX_TOKENS,
                                                temperature: float = DEFAULT_TEMPERATURE,
                                                top_p: float = DEFAULT_TOP_P,
                                                context: str = "æµå¼èŠå¤©",
                                                session_id: str = None,
                                                model: Optional[str] = None,
                                                content_callback: Optional[callable] = None) -> LLMResponse:
        """è°ƒç”¨LLM APIï¼ˆæµå¼ç‰ˆæœ¬ï¼Œæ”¯æŒå›è°ƒå‡½æ•°ï¼‰"""
        # å¼€å§‹AIæŒ‡æ ‡ç›‘æ§
        monitor_id = None
        try:
            result = await call_utcp_tool("ai_metrics_service.start_monitoring", {})
            monitor_id = result.get("monitor_id")
        except Exception as e:
            self.logger.debug(f"å¯åŠ¨AIæŒ‡æ ‡ç›‘æ§å¤±è´¥: {e}")
        start_time = time.time()
        first_token_time = None
        
        try:
            self._log_request_details(messages, context, tools)
            
            # è®¡ç®—è¾“å…¥ç»Ÿè®¡
            total_input_chars = sum(len(str(msg.get('content', ''))) for msg in messages)
            tool_count = len(tools) if tools else 0
            
            self.logger.debug(f"ğŸš€ å¼€å§‹{context}æµå¼APIè°ƒç”¨")
            
            # åŠ¨æ€è·å–æœåŠ¡åç§°ï¼ˆä½¿ç”¨é¢„è§£æçš„é…ç½®ï¼‰
            provider, service_name, stream_service_name, model_name = self._get_service_names(model)
            
            # APIè°ƒç”¨é˜¶æ®µ
            api_params = self._prepare_api_params(messages, tools, max_tokens, temperature, top_p, model_name)
            
            stream_response = await call_utcp_tool_stream(stream_service_name, api_params)
            
            # è®°å½•HTTPé¦–å­—èŠ‚æ—¶é—´
            http_first_byte_time = time.time()
            http_first_byte_duration = (http_first_byte_time - start_time) * 1000
            
            self.logger.debug(f"ğŸŒ HTTPé¦–å­—èŠ‚æ—¶é—´: {http_first_byte_duration:.2f}ms")
            
            # å¤„ç†æµå¼å“åº”ï¼Œæ”¯æŒå›è°ƒå‡½æ•°
            response_data = await self._process_stream_response(
                stream_response, monitor_id, content_callback)
            
            # è®¡ç®—è¾“å‡ºç»Ÿè®¡
            output_chars = len(response_data.get("content", ""))
            tool_calls_made = len(response_data.get("tool_calls", []))
            
            # è®¡ç®—ç¬¬ä¸€ä¸ªtokençš„å»¶è¿Ÿ
            first_token_duration = None
            if response_data.get("first_token_time"):
                first_token_duration = (response_data["first_token_time"] - start_time) * 1000
                self.logger.debug(f"ğŸ¯ ç¬¬ä¸€ä¸ªtokenå»¶è¿Ÿ: {first_token_duration:.2f}ms")
            
            # ä½¿ç”¨é›†æˆçš„å®Œæˆç›‘æ§æ¥å£
            if monitor_id:
                try:
                    await call_utcp_tool("ai_metrics_service.finish_monitoring", {
                        "monitor_id": monitor_id,
                        "provider": provider,
                        "model_name": response_data.get("used_model"),
                        "session_id": session_id,
                        "prompt_tokens": response_data.get("usage", {}).get("prompt_tokens", 0),
                        "completion_tokens": response_data.get("usage", {}).get("completion_tokens", 0),
                        "input_chars": total_input_chars,
                        "output_chars": output_chars,
                        "tool_count": tool_count,
                        "tool_calls_made": tool_calls_made,
                        "http_first_byte_time": http_first_byte_duration,
                        "first_token_time": first_token_duration
                    })
                except Exception as e:
                    self.logger.debug(f"å®ŒæˆAIæŒ‡æ ‡ç›‘æ§å¤±è´¥: {e}")
            
            # ä½¿ç”¨ç»Ÿä¸€çš„å“åº”å¤„ç†æ¥å£
            process_duration = (time.time() - start_time) * 1000
            return await self._handle_llm_response(response_data, context, process_duration)
            
        except Exception as e:
            self.logger.error(f"âŒ {context}æµå¼APIè°ƒç”¨å¤±è´¥: {e}")
            if monitor_id:
                try:
                    await call_utcp_tool("ai_metrics_service.finish_monitoring", {
                        "monitor_id": monitor_id,
                        "provider": provider,
                        "result": str(e)
                    })
                except Exception:
                    pass
            raise
    
    async def _process_tool_calls(self, tool_calls: List[ToolCall], conversation_history: List[Dict[str, Any]]):
        """å¤„ç†å·¥å…·è°ƒç”¨"""
        for tool_call in tool_calls:
            tool_name = tool_call.function_name
            try:
                arguments = json.loads(tool_call.function_arguments)
            except json.JSONDecodeError:
                self.logger.error(f"Invalid JSON in tool arguments: {tool_call.function_arguments}")
                arguments = {}
            
            self.logger.info(f"Calling tool: {tool_name}, args: {arguments}")
            
            try:
                tool_result = await call_utcp_tool(tool_name, arguments)
                result_str = self._format_tool_result(tool_result)
                self.logger.info(f"Tool result: {tool_name}, result: {result_str}")
                
            except Exception as e:
                result_str = f"å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}"
                self.logger.error(f"Tool call failed: {tool_name}, error: {e}")
            
            conversation_history.append({
                "role": "tool",
                "tool_call_id": tool_call.id,
                "content": result_str
            })
    
    def _format_tool_result(self, tool_result: Any) -> str:
        """æ ¼å¼åŒ–å·¥å…·ç»“æœä¸ºå­—ç¬¦ä¸²"""
        if isinstance(tool_result, str):
            return tool_result
        elif isinstance(tool_result, (int, float)):
            return str(tool_result)
        elif isinstance(tool_result, (list, dict)):
            return json.dumps(tool_result, ensure_ascii=False)
        else:
            return str(tool_result)
    
    def _prepare_api_params(self, messages: List[Dict[str, Any]], 
                           tools: Optional[List[Dict[str, Any]]], 
                           max_tokens: int, 
                           temperature: float, 
                           top_p: float,
                           model: str) -> Dict[str, Any]:
        """å‡†å¤‡APIè°ƒç”¨å‚æ•°"""
        api_params = {
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p
        }
        
        if tools:
            api_params["tools"] = tools
            api_params["tool_choice"] = "auto"
        
        api_params["model"] = model
        
        return api_params
    
    def _log_request_details(self, messages: List[Dict[str, Any]], context: str = "LLMè¯·æ±‚", tools: Optional[List[Dict[str, Any]]] = None) -> None:
        """è®°å½•LLMè¯·æ±‚çš„è¯¦ç»†ä¿¡æ¯ç”¨äºè°ƒè¯•"""
        if not messages:
            self.logger.debug("ğŸ“Š æ¶ˆæ¯åˆ—è¡¨ä¸ºç©º")
            return
            
        # åªæœ‰åœ¨debugçº§åˆ«å¯ç”¨æ—¶æ‰è¿›è¡Œjsonåºåˆ—åŒ–
        import logging
        if self.logger.isEnabledFor(logging.DEBUG):
            import json
            self.logger.debug(f"ğŸ“‹ Messages: {json.dumps(messages, ensure_ascii=False, separators=(',', ':'))}")
    
    
    def _process_api_response(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†APIå“åº”"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„APIå“åº”æ ¼å¼è¿›è¡Œå¤„ç†
        # æš‚æ—¶è¿”å›åŸºæœ¬ç»“æ„
        return {
            "content": result.get("content", ""),
            "tool_calls": result.get("tool_calls", []),
            "usage": result.get("usage"),
            "used_model": result.get("used_model", "unknown")
        }
    
    
    async def _process_stream_response(self, stream_response, monitor_id: Optional[str] = None, content_callback: Optional[callable] = None) -> Dict[str, Any]:
        """å¤„ç†æµå¼å“åº”æ•°æ®ï¼Œæ”¯æŒå›è°ƒå‡½æ•°"""
        # ç±»å‹æ£€æŸ¥ï¼šç¡®ä¿ stream_response æ˜¯ StreamResponse å¯¹è±¡
        from src.utcp.streaming import StreamResponse as StreamResponseType
        
        if not isinstance(stream_response, StreamResponseType):
            error_msg = f"stream_response å¿…é¡»æ˜¯ StreamResponse å¯¹è±¡ï¼Œå®é™…ç±»å‹: {type(stream_response)}, å€¼: {stream_response}"
            self.logger.error(error_msg)
            raise TypeError(error_msg)
        
        full_content = ""
        tool_calls = []
        usage_info = None
        used_model = None
        first_token_time = None
        
        async for chunk in stream_response:
            chunk_type = chunk.get("type")
            
            # è°ƒç”¨å›è°ƒå‡½æ•°å¤„ç†å®æ—¶å†…å®¹
            if content_callback:
                try:
                    await content_callback(chunk_type, chunk)
                except Exception as e:
                    self.logger.warning(f"å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
            
            if chunk_type == "content_delta":
                # è®°å½•ç¬¬ä¸€ä¸ªtokençš„æ—¶é—´
                if first_token_time is None:
                    first_token_time = time.time()
                    self.logger.debug(f"ğŸ¯ ç¬¬ä¸€ä¸ªtokenæ—¶é—´: {first_token_time}")
                
                self.logger.debug(f"ğŸ” æµå¼å“åº”å†…å®¹: {chunk}")
                full_content = await self._handle_content_delta(chunk, full_content)
            elif chunk_type == "tool_calls_delta":
                tool_calls = self._handle_tool_calls_delta(chunk, tool_calls)
            elif chunk_type == "completion":
                full_content, tool_calls, usage_info, used_model = self._handle_completion(chunk, full_content, tool_calls)
            elif chunk_type == "error":
                self._handle_stream_error(chunk)
        
        return {
            "content": full_content.strip(),
            "tool_calls": tool_calls,
            "usage": usage_info,
            "used_model": used_model,
            "first_token_time": first_token_time
        }
    
    async def _handle_llm_response(self, response_data: Dict[str, Any], 
                                 context: str, process_duration: float) -> LLMResponse:
        """ç»Ÿä¸€çš„LLMå“åº”å¤„ç†æ–¹æ³•"""
        
        # æå–å“åº”æ•°æ®
        content = response_data.get("content", "")
        tool_calls = response_data.get("tool_calls", [])
        usage_info = response_data.get("usage")
        used_model = response_data.get("used_model")
        request_model = response_data.get("request_model")
        
        # è®°å½•åŸºæœ¬çš„å®Œæˆä¿¡æ¯
        content_length = len(content)
        tool_call_count = len(tool_calls)
        
        # æ„å»ºæ—¥å¿—æ¶ˆæ¯
        self.logger.info(f"{context} - è€—æ—¶: {process_duration:.2f}ms, å­—ç¬¦æ•°: {content_length}, å·¥å…·è°ƒç”¨: {'æ˜¯' if tool_call_count > 0 else 'å¦'} ({tool_call_count}ä¸ª)ï¼Œå†…å®¹ï¼š{content}")
        
        return LLMResponse.create(content, tool_calls, usage_info, used_model, request_model)
    
    
    async def _handle_content_delta(self, chunk: Dict[str, Any], full_content: str) -> str:
        """å¤„ç†å†…å®¹å¢é‡"""
        delta_content = chunk.get("delta", {}).get("content", "")
        if delta_content:
            full_content = chunk.get("full_content", full_content + delta_content)
        return full_content
    
    def _handle_tool_calls_delta(self, chunk: Dict[str, Any], tool_calls: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤„ç†å·¥å…·è°ƒç”¨å¢é‡"""
        tool_call_deltas = chunk.get("delta", {}).get("tool_calls", [])
        
        for tool_call_delta in tool_call_deltas:
            index = tool_call_delta.get("index", 0)
            
            # ç¡®ä¿tool_callsåˆ—è¡¨è¶³å¤Ÿé•¿
            while len(tool_calls) <= index:
                tool_calls.append({})
            
            # æ›´æ–°å·¥å…·è°ƒç”¨ä¿¡æ¯
            self._update_tool_call_at_index(tool_calls, index, tool_call_delta)
        
        return tool_calls
    
    def _update_tool_call_at_index(self, tool_calls: List[Dict[str, Any]], index: int, delta: Dict[str, Any]) -> None:
        """æ›´æ–°æŒ‡å®šç´¢å¼•çš„å·¥å…·è°ƒç”¨ä¿¡æ¯"""
        if "id" in delta:
            tool_calls[index]["id"] = delta["id"]
        if "type" in delta:
            tool_calls[index]["type"] = delta["type"]
        if "function" in delta:
            if "function" not in tool_calls[index]:
                tool_calls[index]["function"] = {}
            
            function_delta = delta["function"]
            if "name" in function_delta:
                tool_calls[index]["function"]["name"] = function_delta["name"]
            if "arguments" in function_delta:
                current_args = tool_calls[index]["function"].get("arguments", "")
                tool_calls[index]["function"]["arguments"] = current_args + function_delta["arguments"]
    
    def _handle_completion(self, chunk: Dict[str, Any], full_content: str, tool_calls: List[Dict[str, Any]]) -> tuple:
        """å¤„ç†å®Œæˆä¿¡æ¯"""
        final_content = chunk.get("content", "")
        final_tool_calls = chunk.get("tool_calls")
        usage_info = chunk.get("usage")
        used_model = chunk.get("used_model")
        
        # ä½¿ç”¨æœ€ç»ˆæ•°æ®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if final_content:
            full_content = final_content
        if final_tool_calls:
            tool_calls = final_tool_calls
        
        return full_content, tool_calls, usage_info, used_model
    
    def _handle_stream_error(self, chunk: Dict[str, Any]) -> None:
        """å¤„ç†æµå¼é”™è¯¯"""
        error_msg = chunk.get("error", "æµå¼å“åº”å¤„ç†é”™è¯¯")
        self.logger.error(f"æµå¼å“åº”é”™è¯¯: {error_msg}")
        raise Exception(error_msg)
